#!/usr/bin/env python3

import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def download_images(current_url, current_depth, recursive, path, total_recursion, urls):
	urls.append(current_url.rstrip('/'))
	if current_depth < 0:
		return
	try:
		response = requests.get(current_url)
		response.raise_for_status()
	except requests.exceptions.RequestException as e:
		print(f"Failed to fetch {current_url}: {e}")
		return
	
	soup = BeautifulSoup(response.text, 'html.parser')
	images = soup.find_all('img', src=True)
	
	for img in images:
		img_url = urljoin(current_url, img['src'])
		if any(img_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']):
			try:
				img_data = requests.get(img_url).content
				temp_name = f"{total_recursion}_{os.path.basename(img_url)}"
				img_name = os.path.join(path, temp_name)
				base_name, ext = os.path.splitext(img_name)
				counter = 1
				unique_name = img_name
				while (os.path.exists(unique_name)):
					unique_name = f"{base_name}_{counter}{ext}"
					counter += 1
				with open(unique_name, 'wb') as f:
					f.write(img_data)
					print(f"Downloading image: {img_url}")
			except Exception as e:
				print(f"Failed to download {img_url} because of: {e}")
	
	if recursive and current_depth > 0:
		links = soup.find_all('a', href=True)
		for link in links:
			new_url = urljoin(current_url, link['href'])
			if new_url.rstrip('/') in urls:
				print("Skipping: " + new_url)
				continue
			print("Downloading from url: " + new_url)
			download_images(new_url, current_depth - 1, recursive, path, total_recursion + 1, urls)

def main():
	total_recursion = 0
	depth = 0
	recursive = False
	path = "./data"

	args = sys.argv[1:]
	for arg in args:
		if arg.startswith('-') and len(arg) == 2:
			if not arg in ['-r', '-l', '-p']:
				print("Invalid option {arg}")
				return
			elif args.count(arg) != 1:
				print("Repetative option {arg}")
				return
	try:
		if '-r' in args:
			recursive = True
			depth = 5
		if '-l' in args:
			if '-r' in args:
				depth = int(args[args.index('-l') + 1])
			else:
				print("Error! -l should be used with -r")
				return
		if '-p' in args:
			path = args[args.index('-p') + 1]
	except Exception as e:
		print(f"Invalid argument due to {e}")

	url = args[-1]
	if not url.startswith('http'):
		print("Invalid URL provided.")
		return
	os.makedirs(path, exist_ok=True)
	urls = []
	download_images(url, depth, recursive, path, total_recursion, urls)
	print(f"\nDownloaded images to {path} from:\n {urls}")

if __name__ == "__main__":
	main()
